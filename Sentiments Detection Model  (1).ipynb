{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13dc825",
   "metadata": {},
   "source": [
    "# End-to-End Data Science Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26efae1",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689d312",
   "metadata": {},
   "source": [
    "In this Project, I downloaded Black Panther 18 dataset from kaggle, performed a sentiment analysis on the data using the VaderSentiment library and then used it to train a sentiment detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4934c7f",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Introduction\n",
    "2. Data source\n",
    "3. Data Quality Assessment and Cleaning\n",
    "4. Sentiment Analysis\n",
    "5. Data Spltting\n",
    "6. Model Selection and Training\n",
    "7. Model Evaluation\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d78f40",
   "metadata": {},
   "source": [
    "# 2. Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2504e",
   "metadata": {},
   "source": [
    "I downloaded Black Panther 18 dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1ac7fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing liberies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import cleantext\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from cleantext import clean\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import warnings \n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f63a8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data frame\n",
    "DataFrame=pd.read_csv(\"Black Panther.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "928e54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting tweets in english only\n",
    "df=DataFrame[DataFrame['Language']=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a07afa",
   "metadata": {},
   "source": [
    "# 3. Data Quality Assessment And Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c987ab8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64010"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect length of the dataset\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cfcc54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping dupliacated values\n",
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2690d56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49298"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect length of the dataset after dropping duplicates\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "04bcf780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49298 entries, 0 to 57116\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Tweets     49298 non-null  object\n",
      " 1   User_name  49298 non-null  object\n",
      " 2   Language   49298 non-null  object\n",
      " 3   Location   34997 non-null  object\n",
      " 4   Time       49298 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#Inspect information of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b5ad0488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        RT @CoachWilmore: #120: William OÕNeal and the...\n",
       "1        RT @soprettyinlou: I hope my girl Shuri can br...\n",
       "2        RT @PollsNig: Ok guys get in here, who do you ...\n",
       "3        the thing is... black panther was so so good b...\n",
       "4        RT @HillaryClinton: Saw Black Panther with Bil...\n",
       "                               ...                        \n",
       "57097                                    _ BLACK PANTHER _\n",
       "57100    ___ my fav character was Shuri! https://t.co/j...\n",
       "57103        ____ #wakanda forever https://t.co/Whxkh1TdTG\n",
       "57112    _____&lt;ÑÑÑ we got the official Wakanda emoji...\n",
       "57116                   _____ girl https://t.co/wDQr9Zrqnx\n",
       "Name: Tweets, Length: 49298, dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect tweets to identify data quality issues before cleaning\n",
    "df['Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "093d3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning tweeter data \n",
    "def text_process(tweet):\n",
    "    #Converting tweets to lowercase \n",
    "    tweet=tweet.lower()\n",
    "    #Removing emojis\n",
    "    clean(tweet, no_emoji=True)\n",
    "    #Removing URL's\n",
    "    tweet=re.sub(r\"http\\S+|www\\S+|https\\S+\",'',tweet,flags=re.MULTILINE)\n",
    "    #Removing repeating characters\n",
    "    tweet=re.sub(r'\\@\\w+|\\#\\w+|\\d+\\rt', '', tweet)\n",
    "    #Removing rt\n",
    "    tweet=re.sub('rt','',tweet)\n",
    "    #Removing underscores\n",
    "    tweet=re.sub('_','',tweet)\n",
    "    tweet=re.sub('__','',tweet)\n",
    "    tweet=re.sub('___','',tweet)\n",
    "    tweet=re.sub('____','',tweet)\n",
    "    #Removing stopwords\n",
    "    tokens=nltk.word_tokenize(tweet)\n",
    "    filted_words=[w for w in tokens if w not in stopwords.words('english')]\n",
    "    #Removing punctuations\n",
    "    nopunc=[w for w in filted_words if w not in string.punctuation]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in nopunc]\n",
    "    return \" \".join(lemma_words)\n",
    "#Applying text_process function to the data frame\n",
    "df['Tweets']=df['Tweets'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5c21706d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User_name</th>\n",
       "      <th>Language</th>\n",
       "      <th>Location</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>william oõneal murder fred hampton william oõn...</td>\n",
       "      <td>SusieNattibree</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Mar 04 10:28:35 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hope girl shuri bring back erik round 2 black ...</td>\n",
       "      <td>zinedine_7x</td>\n",
       "      <td>en</td>\n",
       "      <td>P</td>\n",
       "      <td>Sun Mar 04 10:28:35 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok guy get think would win battle okoye black ...</td>\n",
       "      <td>edxxtrock</td>\n",
       "      <td>en</td>\n",
       "      <td>Toulouse, France</td>\n",
       "      <td>Sun Mar 04 10:28:35 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing ... black panther good session late woke...</td>\n",
       "      <td>zekejaegers</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Mar 04 10:28:36 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saw black panther bill afternoon amp loved bea...</td>\n",
       "      <td>quirion77</td>\n",
       "      <td>en</td>\n",
       "      <td>Loire-Atlantique, Pays de la Loire</td>\n",
       "      <td>Sun Mar 04 10:28:36 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57097</th>\n",
       "      <td>black panther</td>\n",
       "      <td>icechology</td>\n",
       "      <td>en</td>\n",
       "      <td>city of the goddesses</td>\n",
       "      <td>Sun Mar 04 03:40:32 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57100</th>\n",
       "      <td>fav character shuri</td>\n",
       "      <td>mc_magic1887</td>\n",
       "      <td>en</td>\n",
       "      <td>_</td>\n",
       "      <td>Sun Mar 04 03:26:18 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57103</th>\n",
       "      <td>forever</td>\n",
       "      <td>jamaa222</td>\n",
       "      <td>en</td>\n",
       "      <td>Mombasa, Kenya</td>\n",
       "      <td>Sun Mar 04 04:09:06 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57112</th>\n",
       "      <td>lt ñññ got official wakanda emoji right</td>\n",
       "      <td>var_aleti</td>\n",
       "      <td>en</td>\n",
       "      <td>not in school</td>\n",
       "      <td>Sun Mar 04 03:20:06 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57116</th>\n",
       "      <td>girl</td>\n",
       "      <td>_Cocaine_Caviar</td>\n",
       "      <td>en</td>\n",
       "      <td>eastside chicaGLO__ Blono</td>\n",
       "      <td>Sun Mar 04 04:02:23 +0000 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49298 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweets        User_name  \\\n",
       "0      william oõneal murder fred hampton william oõn...   SusieNattibree   \n",
       "1      hope girl shuri bring back erik round 2 black ...      zinedine_7x   \n",
       "2      ok guy get think would win battle okoye black ...        edxxtrock   \n",
       "3      thing ... black panther good session late woke...      zekejaegers   \n",
       "4      saw black panther bill afternoon amp loved bea...        quirion77   \n",
       "...                                                  ...              ...   \n",
       "57097                                      black panther       icechology   \n",
       "57100                                fav character shuri     mc_magic1887   \n",
       "57103                                            forever         jamaa222   \n",
       "57112            lt ñññ got official wakanda emoji right        var_aleti   \n",
       "57116                                               girl  _Cocaine_Caviar   \n",
       "\n",
       "      Language                            Location  \\\n",
       "0           en                                 NaN   \n",
       "1           en                                   P   \n",
       "2           en                    Toulouse, France   \n",
       "3           en                                 NaN   \n",
       "4           en  Loire-Atlantique, Pays de la Loire   \n",
       "...        ...                                 ...   \n",
       "57097       en               city of the goddesses   \n",
       "57100       en                                   _   \n",
       "57103       en                      Mombasa, Kenya   \n",
       "57112       en                       not in school   \n",
       "57116       en           eastside chicaGLO__ Blono   \n",
       "\n",
       "                                 Time  \n",
       "0      Sun Mar 04 10:28:35 +0000 2018  \n",
       "1      Sun Mar 04 10:28:35 +0000 2018  \n",
       "2      Sun Mar 04 10:28:35 +0000 2018  \n",
       "3      Sun Mar 04 10:28:36 +0000 2018  \n",
       "4      Sun Mar 04 10:28:36 +0000 2018  \n",
       "...                               ...  \n",
       "57097  Sun Mar 04 03:40:32 +0000 2018  \n",
       "57100  Sun Mar 04 03:26:18 +0000 2018  \n",
       "57103  Sun Mar 04 04:09:06 +0000 2018  \n",
       "57112  Sun Mar 04 03:20:06 +0000 2018  \n",
       "57116  Sun Mar 04 04:02:23 +0000 2018  \n",
       "\n",
       "[49298 rows x 5 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if data has been cleaned as expected\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f23461",
   "metadata": {},
   "source": [
    "# 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7424e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer=SentimentIntensityAnalyzer()\n",
    "df['scores']=df['Tweets'].apply(lambda text: analyzer.polarity_scores(text) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7e6ea702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the polarity\n",
    "def sentimentpredict(sentiment):\n",
    "    if sentiment['compound']>=0.05:\n",
    "        return \"Positive\"\n",
    "    elif sentiment['compound']<=-0.05: \n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "df['label']=df['scores'].apply(lambda x: sentimentpredict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a7464640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['User_name','Language','Time','scores','Location'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "307ab242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>william oõneal murder fred hampton william oõn...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hope girl shuri bring back erik round 2 black ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok guy get think would win battle okoye black ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing ... black panther good session late woke...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saw black panther bill afternoon amp loved bea...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57097</th>\n",
       "      <td>black panther</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57100</th>\n",
       "      <td>fav character shuri</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57103</th>\n",
       "      <td>forever</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57112</th>\n",
       "      <td>lt ñññ got official wakanda emoji right</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57116</th>\n",
       "      <td>girl</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49298 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweets     label\n",
       "0      william oõneal murder fred hampton william oõn...  Negative\n",
       "1      hope girl shuri bring back erik round 2 black ...  Positive\n",
       "2      ok guy get think would win battle okoye black ...  Positive\n",
       "3      thing ... black panther good session late woke...  Positive\n",
       "4      saw black panther bill afternoon amp loved bea...  Positive\n",
       "...                                                  ...       ...\n",
       "57097                                      black panther   Neutral\n",
       "57100                                fav character shuri  Positive\n",
       "57103                                            forever   Neutral\n",
       "57112            lt ñññ got official wakanda emoji right   Neutral\n",
       "57116                                               girl   Neutral\n",
       "\n",
       "[49298 rows x 2 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7f5eb",
   "metadata": {},
   "source": [
    "# 5. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "015240dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8bde4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, label_train, label_test = train_test_split(df['Tweets'],df['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f7dc8",
   "metadata": {},
   "source": [
    "# 6.Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e48db3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9422373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ff24f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "280153bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "70956417",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a4c1211e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;bow&#x27;,\n",
       "                 CountVectorizer(analyzer=&lt;function text_process at 0x000001E22832BC70&gt;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;bow&#x27;,\n",
       "                 CountVectorizer(analyzer=&lt;function text_process at 0x000001E22832BC70&gt;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function text_process at 0x000001E22832BC70&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x000001E22832BC70>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(text_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "95950af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pipeline.predict(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c4f00",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6c9194a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "78a0ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.57      0.69      4359\n",
      "     Neutral       0.67      0.25      0.37      5114\n",
      "    Positive       0.53      0.91      0.67      6796\n",
      "\n",
      "    accuracy                           0.61     16269\n",
      "   macro avg       0.70      0.58      0.58     16269\n",
      "weighted avg       0.67      0.61      0.58     16269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b45e5c",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f739e",
   "metadata": {},
   "source": [
    "The model is pretty good with 61% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7091e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
